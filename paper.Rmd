---
title: "Asymmetric continuous prediction bands for assessing the validity of smooth biomechanical curve data"
author:
  - name: Daniel Koska
    email: daniel.koska@hsw.tu-chemnitz.de
    affiliation: Chemnitz University of Technology, Research Methodology and Data Analysis in Biomechanics
    footnote: Corresponding Author
  - name: Doris Oriwol
    email: doris.oriwol@partner.kit.edu
    affiliation: Karlsruhe Institute of Technology
  - name: Christian Maiwald
    email: christian.maiwald@hsw.tu-chemnitz.de
    affiliation: Chemnitz University of Technology, Research Methodology and Data Analysis in Biomechanics
# address:
#   - code: Chemnitz University of Technology
#     address:  Thüringer Weg 11, 09126 Chemnitz
#   - code: Another University
#     address: Department, Street, City, State, Zip
abstract: |
  Paper als short communication / technical note?

date: "`r format(Sys.time(), '%d %B, %Y')`"
journal: "An awesome journal"
# bibliography: sendaFAB.bib
# output: rticles::elsevier_article
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
---

### TODO
- Exemplarische Paper für die jeweiligen simulierten Datensätze raussuchen
- Weitere synthetische Daten siehe Paper Robinson et al. (2021)
- Evtl. simulierte Datensätze um Beispiele aus https://mjskay.github.io/ggdist/articles/lineribbon.html erweitern (insbesondere das erste Beispiel mit x-versetzten Gausskurven)!
- Simulierte Daten durch Funktionen ersetzen? Eigentlich aktuell eher unnötig, da die Zeitreihen ja eigentlich bereits mithilfe von Funktionen modelliert wurden. Zudem wird nicht interpoliert, abgeleitet o. Ä. ... letztlich für die aktuelle Idee des Papers eher ungeeignet (plus man umgeht Probleme wie mögliche Glättungsartefakte, oder die Definition eines functional mean).
- Falls doch FDA, dann evtl. als eigenständiges Skript forken
- Begründung warum Prädiktionsintervalle anstelle von Konfidenzintervallen
<!-- - Umbennung entire curve in "All-or-nothing" (entlehnt von Juul et al.) -->
- .bib file anlegen
- Begriff spatiotemporal überdenken (wie ist das im Kontext FDA definiert)
- Ratkowsky besorgen
- Begründung warum Prädiktionsintervalle und nicht Konfidenzintervalle (theoretisch könnte man beides machen; Prädiktionsintervalle scheinen mir aber eher das zu sein, was man will)

### TODO read (again)
- Robinson, Vanrenterghem, Pataky (2021) Sample size estimation for biomechanical waveforms: Current practice, recommendations and a comparison to discrete power analysis
- Olshen et al. (1989) Gait analysis and the bootstrap
- Curvewise point and interval summaries for tidy data frames of draws from distributions https://mjskay.github.io/ggdist/reference/curve_interval.html
- Horvath, Kokoszka & Reeder (2013) Estimation of the mean of functional time series and a two-sample problem
- Generell die Kooperationspaper von Hörmann und Kokoszka zu FDA Statistik,insbesondere 'Inference for Functional Data with Applications' (2012)
- Ratkowsky 1990 Handbook of nonlinear regression models


```{r echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)

dir.script <- "~/floa/R"
dir.data <- "~/floa/R/examples"

source(paste0(dir.script, "/example_data.R"))
source(paste0(dir.script, "/pick_subwise_curves.R"))
source(paste0(dir.script, "/draw_clusters.R"))
source(paste0(dir.script, "/functional_mean.R"))
source(paste0(dir.script, "/functional_sd.R"))
source(paste0(dir.script, "/boot_mean_sd.R"))
source(paste0(dir.script, "/floa_rcb.R"))
source(paste0(dir.script, "/floa_point.R"))
source(paste0(dir.script, "/floa_roislien.R"))
source(paste0(dir.script, "/plot_loa.R"))
source(paste0(dir.script, "/get_coverage.R"))
source(paste0(dir.script, "/get_coverage_singlecurve.R"))
source(paste0(dir.script, "/get_coverage_fraction.R"))
source(paste0(dir.script, "/get_coverage_singlecurve_fraction.R"))
source(paste0(dir.script, "/crossval_coverage.R"))
source(paste0(dir.script, "/singlecurve_coverage.R"))
source(paste0(dir.script, "/crossval_coverage_fraction.R"))
source(paste0(dir.script, "/singlecurve_coverage_fraction.R"))
source(paste0(dir.script, "/plot_cov_ver.R"))
```

# Introduction

<!-- Having a good estimate of the amount of measurement error is a basic requirement when analyzing movement data. The classic linear error model describes measurement error as the sum of systematic and random deviations from the true value. A complete characterization of the measurement error, however, is complex, as different sources of variance (within, between) and noise must be considered. The analysis of sensor data is further complicated by the fact that the output of many measurement systems in biomechanics - such as e. g. joint angles, ground reaction forces, or center of mass trajectories - is available in the form of continuous signals (curve data) within which the error can vary over time. -->

Hier ein anderes Intro schreiben ... Ansatz: Neben der Validierung von Messgeräten spielt auch die Validierung von Modellergebnissen eine Rolle!

Therefore, when analyzing the validity of curve data, the entire length of the curve should be accounted for. This can be accomplished e. g. by applying statistics separately to all points of the curve [Pini et al. (2019). The pointwise approach allows for an error representation over the entire domain, but ignores the fact, that the sampling points within a curve are locally correlated (Deluzio & Astephen, 2007; Pataky, 2010). From a statistical point of view, this implies that the random error component is likely underrepresented when the (nonzero) covariance term of adjacent sampling points is not accounted for (Lenhoff et al., 1999).

<!-- ... and likely results in a systematic underrepresentation of ... values (Juul et al., 2020; Mirzagar et al. (2014). -->

<!-- https://mjskay.github.io/ggdist/reference/curve_interval.html -->
<!-- Hier referenzierte Paper u. a.  -->
<!-- - Juul Jonas, Kaare Græsbøll, Lasse Engbo Christiansen, and Sune Lehmann. (2020). "Fixed-time descriptive statistics underestimate extremes of epidemic curve ensembles". arXiv e-print. arXiv:2007.05035 -->
<!-- - Mirzargar, Mahsa, Ross T Whitaker, and Robert M Kirby. (2014). "Curve Boxplot: Generalization of Boxplot for Ensembles of Curves". IEEE Transactions on Visualization and Computer Graphics. 20(12): 2654-2663. doi: 10.1109/TVCG.2014.2346455 -->

From a statistical point of view, this implies that the random error component is likely underrepresented when the (nonzero) covariance term of adjacent sampling points is not accounted for (Lenhoff et al., 1999). In addition, most traditional point statistics are either not suitable (e.g. statistical tests of mean differences), or are only suitable to a limited extent (e.g. product-moment correlation) to quantify the agreement between two measurement systems (Bland and Altman, 1986).

One possibility to address the problem of pointwise analysis is to treat entire curves as (functional) objects rather than as a series of unconnected points.

<!-- Es gibt eine Reihe von Ansätzen aus anderen Bereichen (z. B. Covid Prädiktion), bei denen sog. Function oder Curve Boxplots eingesetzt werden. -->
<!-- https://mjskay.github.io/ggdist/reference/curve_interval.html -->
<!-- Hier referenzierte Paper u. a.  -->
<!-- - Juul Jonas, Kaare Græsbøll, Lasse Engbo Christiansen, and Sune Lehmann. (2020). "Fixed-time descriptive statistics underestimate extremes of epidemic curve ensembles". arXiv e-print. arXiv:2007.05035 -->
<!-- - Mirzargar, Mahsa, Ross T Whitaker, and Robert M Kirby. (2014). "Curve Boxplot: Generalization of Boxplot for Ensembles of Curves". IEEE Transactions on Visualization and Computer Graphics. 20(12): 2654-2663. doi: 10.1109/TVCG.2014.2346455 -->

Sutherland et al. (1988) introduced a method in which continuous signals are reconstructed using Fourier series and bootstrapped to estimate of the variation of the sample mean and thus estimate confidence intervals. Lenhoff et al. (1999) illustrated their method on a generic gait analysis data set (comprised of joint angle curves) and compared it to pointwise Gaussian intervals. The authors found that Sutherland et al. bootstrap bands provide appropriate coverage for continuous curve gait data (86% coverage for a targeted coverage of 90%) while pointwise Gaussian bands are shown to provide inadequate coverage (54% for a targeted coverage of 90%).

Røislien et al. (2012) presented a similar approach to calculate a functional version of the the Limits of Agreement (LoA) method suggested by Bland & Altman. They first calculate functional difference curves which are then bootstrapped to obtain 95% confidence intervals. The approach accounts for the autocorrelation characteristic in the signal and at the same time allows for an intuitive description of the systematic and random error terms in the original sampling space (e.g. actual degrees). Røislien et al., however, rely on a number of strong model assumptions, which are often not or only partially fulfilled in empirical data sets:

(i) The error distribution is approximately normal.

The construction of symmetric prediction bands as described in Røislien et al. [10] relies on Gaussian assumptions. Non-normality, however, is often present when e.g. sensor values are truncated because of limited range or resolution, or because values drift over time.

(ii) Time-continuous differences of two measurement systems and the corresponding averages must be uncorrelated.

This assumption is violated considerably in many validation data sets [12]. E.g., Atallah et al. [13] found that the agreement between an ear-worn accelerometer and a force-plate instrumented treadmill was associated with gait speed when estimating gait cycle duration. When validating a foot-worn gyroscope for treadmill running, Koska & Maiwald [14] observed a linear error trend that was associated with foot strike behavior.

(iii) Aus einem Datensatz bestehend aus einer Kurve eines Probanden lassen sich durch Bootstrapping von mittleren Kurven Unsicherheitsintervalle bilden, die ein geeignetes Maß für die estimation uncertainty darstellen.

Hier werden arithmetische mean curves verwendet, the variability of which will reflect variation between means. In many cases, however, it is the variability of individual values that is relevant, and the limits will be too narrow by a factor of approximately the square root of the number of curves within the sample.

Ein weiteres Problem ist, dass die Methode keine messwiederholten Daten berücksichtigt. Da sich messwiederholte Kurven i. d. R. ähnlicher sind als unabhängige Kurven, besteht die Gefahr die Streuung des Messsystems zu überschätzen.

<!-- Anwendung der Argumentation der Reviewer* auf die "Roislien_FLoA": In Roislien et al. werden 95% Konfidenzintervalle gebootstrapped. The authors are producing the arithmetic mean curves, the variability of which will reflect variation between means. It is the variability of individual values that is relevant, and the limits will be too narrow by a factor of approximately the square root of n.subj. -->

<!-- Formulierung aus dem alten Skript -->
<!-- (iii) Measurement uncertainty can be estimated adequately by bootstrapping the error variance from a sample that consists of a single curve per subject. -->
<!-- If no repeated measurements are included in the sample, an essential error component (intra-individual variance) is ignored. The bootstrap – like any other statistical procedure – cannot solve this problem, as it can only estimate proportions of variance that are contained in the sample. -->

<!-- Weiterer ("zugehöriger) Reviewerkommentar: -->
<!-- In looking at limits of agreement, the authors are considering the extent of within pair variation. However, in looking at the point wise limits of agreement, these are defined to incorporate a between pair standard deviation. Even if it was appropriate to utilise between pair variability, the authors should note that it is variances that are additive and not SDs. Perhaps the authors' lack of understanding comes from their erroneous assumption (iii).  -->

<!-- Olsen et al. [15] examined the validity of an inertial measurement unit (IMU) for analyzing the spatial displacement of the distal limbs of horses using functional LoA (FLoA). Their approach differs from Røislien et al. [10] in that initially linear mixed models were fitted to the repeated measurements of different horses. Functional data estimates of the mean trends in the strides were then determined from the model fits and FLoA were calculated. This procedure accounts for repeated measurements and eliminates the need to emulate empirical curves with Fourier series or splines. The FLoA computed this way performed well in comparison with traditional Bland & Altman LoA. The procedure, however, is subject to the same restrictions as any conventional linear regression model and is therefore limited when model assumptions are violated. -->



<!-- Warum eigentlich Prädiktionsbänder und keine Konfidenzbänder? (Siehe Einleitung Lenhoff) -->
<!-- Aktuell würde ich es so formulieren: In der Mehrheit der Fälle ist man an der nächsten (Einzel)Observation interessiert, nicht unbedingt an Mittelwerten (obwohl es durchaus Szenarien gibt, wo das akzeptabel sein kann) + im Zweifel generell lieber breitere Fehlerintervalle -->

The principal object of this paper was to demonstrate a method for constructing continuous prediction intervals from curve data that addresses the aforementioned issues (analyzing the whole curve, including repeated measures, model assumptions (normal distribution, equal variances)), and thus enables a more appropriate characterization of the agreement between measurement curves on the basis of easy to interpret error bands (i.e. LoA in the actual measurement unit).

<!-- Es gibt bereits Ansätze für funktionale Boxplots (inkl. Vergleich mit punktweisen Intervallen), das Ganze wurde aber noch nie für Differenzenkurven untersucht. -->

We use synthetic time series data sets containing typical curves of biomechanical measurement systems to illustrate the method and compare the resulting prediction bands to pointwise Bland & Altman parameters and LoA constructed similar to the method described in Roislien et al. (2012). For model validation, the respective LoA are cross validated using a leave-one-out approach.

# Methods

## Limits of agreement methods

FLoA derived by different methods are compared

* Randomized Cluster Bootstrap (FLoA~RCB~)
  + n = length(subjects) random strides from all strides (FLoA~RCB_v1~)
  + One random stride per subject (FLoA~RCB_v2~)
  + Fetch a SINGLE random stride from all strides (FLoA~RCB_v3~)

* Pointwise Gaussian intervals (FLoA~Point~)
  + FLoA~Point~ are calculated using mean and standard deviations (derived from linear mixed effects models)

* Roislien-like intervals (FLoA~Roslien~)
  + (Get one random stride from each subject ONCE and boot-
       strap the resulting sample (of length (n=length(subjects))
       
### FLoA~RCB~ construction

<!-- See Mirzargar et al. (2014) or Juul et al. (2020) for an accessible introduction to data depth and curve boxplots / functional boxplots. -->


```{r echo = FALSE, warning = FALSE, message = FALSE}
# Wrapper function for example data sets. Function arguments:
#
# (* Empirical validation data: "imu_mc")
# * Smooth, wave data (normal error, constant variance, no trend): "smooth"
# * Smooth wave data with nonlinear trend (constant variance): "smooth_trend"
# * Data with non-gaussian (Weibull distributed) error (no trend): "non_gaussian"
# * Data with shock peaks (no bias, no trend): "shock"

data <- example_data(dat = "smooth", dir.data)

# Mean and SD are calculated across all strides (and subjects).
# No bootstrap or other resampling strategies are applied.
floa.point <- floa_point(data)
```

## Validation

### Synthetic data sets

To compare different methods for constructing continuous LoA, several sets of smooth curve data were simulated across a range of common biomechanical signals and error characteristics. The quality of the curves is similar, as the paper is primarily developed for a validation context in which the same movement process is analyzed using similar methods (two measurement devices or models). For the same reason, the curves do not necessarily correspond to real-world signals as we were more concerned with illustrating the respective methods and error characterstics.

All curves were modeled as Fourier series from additively superimposed sine functions. Each data set contains n.strides = 10 curves from n.subj = 11 fictitious test subjects. Each curve consists of t = 101 data points. 

<!-- Aus Robinson et al. (2021) -->
<!-- Exemplar 1D effects from the biomechanics literature across a range of signal types were collated and reported (Bakke & Besier, 2020, Barrios & Willson, 2017, Bovi, Rabuetti, Mazzoleni, & Ferrarin, 2011, Phan et al., 2017, Gomes, Ackermann, Ferreira, Orselli, & Sacco, 2017, Robinson et al., 2014). -->

<!-- the code to reproduce all 1D effects is provided at https://github.com/m-a-robinson/sample-size -->

```{r warning = FALSE, message = FALSE, eval = FALSE}
n.subj <- 11
n.ts <- 100

t <- seq(0, 100)

for (subject.idx in 1:n.subj) {

  # Subjectwise parameters
  offset.mean <- runif(1, min = -0.5, max = 0.5)

  a.sd <- runif(1, min = 0.05, max = 0.15)
  b.sd <- runif(1, min = 0.0001, max = 0.002)

  for (stride.idx in 1:(n.strides)) {

    a1.device1 <- rnorm(1, mean = 3, sd = a.sd)
    a1.device2 <- rnorm(1, mean = 3, sd = a.sd)
    a2.device1 <- 0.08
    a2.device2 <- 0.08
    b1.device1 <- rnorm(1, mean = 0.06, sd = b.sd)
    b1.device2 <- rnorm(1, mean = 0.06, sd = b.sd)
    b2.device1 <- rnorm(1, mean = 0.58, sd = b.sd)
    b2.device2 <- rnorm(1, mean = 0.58, sd = b.sd)
    c <- 2

    sine.1 <- a1.device1 * sin(b1.device1 * t) ^ (c + 3)
    sine.2 <- a2.device1 * sin(b2.device1 * t)
    sine.3 <- a1.device2 * sin(b1.device2 * t) ^ (c + 3)
    sine.4 <- a2.device2 * sin(b2.device1 * t)

    offset <- rnorm(1, offset.mean, 0.05)
    
    # Model (i). (ii), or (iii)
    # ...
```

* (1) Homoscedastic (normal) error. Within and between subject differences between curves were modeled using random systematic offset (bias) and random variation of curve parameters. The data are representative of e. g. joint angle measurements simultaneously recorded from two kinematic measurement systems (e. g. a camera-based measurement systems and a goniometer).

```{r warning = FALSE, message = FALSE, eval = FALSE}
# ...

# Model (i)
device.1 <- sine.1 + sine.2
device.2 <- offset + sine.3 + sine.4
```

* (2) Trend error: Same model as in (i), but with a nonlinear trend underlying one of the two signals. Similar errors can be observed e. g. when joint angles are calculated (numerically integrated) from gyroscope signals.

```{r warning = FALSE, message = FALSE, eval = FALSE}
# ...
trend <- (1 / 100000) * seq(0.5, 50.5, 0.5)^3

# Model (ii)
device.1 <- sine.1 + sine.2
device.1 <- offset + sine.3 + sine.4 + trend
```

* (3) Heteroscedastic (non-gaussian, Weibull distributed) error: Heteroscedasticity across the signal length. The example consists of knee moment curves calculated using two different methods (direct and inverse kinematic approach) (Robinson et al., 2014). The curves were recreated using the example code provided in Robinson et al. (2021).

<!-- Robinson et al. (2014) Impact of Knee Modeling Approach on Indicators and Classification of Anterior Cruciate Ligament Injury Risk -->

<!-- Smooth curves created using the same model and parameters as in (i), but Weibulll distributed random curve (amplitude) parameters. This introduces non-gaussian measurement error (asymmetric/skewed distribution of differences around the mean difference). -->

```{r warning = FALSE, message = FALSE, eval = FALSE}
# ...
a1.device2 <- rnorm(n = 1,
                    mean = rweibull(1, shape = 1.5, scale=1) - factorial(1/1.5), # factorial() used to center around 0
                    sd = a.sd)
# ...
sine.3 <- a1.device2 * sin(b1.device2 * t) ^ (c + 3)
# ...

# Model (iii)
device.1 <- sine.1 + sine.2
device.2 <- offset + sine.3 + sine.4
```

* (4) Data with (missing) peaks. Same model as in (i), except a couple of frames where a short, prominent peak occurs in one of the two signals. This typically happens in high-frequency signals such as accelerations or ground reaction forces, where peaks are underrepresented or entirely missing when e. g. sampling rates are too low or lowpass filter artefacts occur. Also, modelling these kind of abrupt changes in the signal (when e. g. to estimate ground reaction forces from acceleration values) is more difficult and my result in similar errors (Alcantara, 2021). 

<!-- Alcantara et al. Modelling/estimating ground reaction forces from acceleration values, z. B. https://www.biorxiv.org/content/10.1101/2021.03.17.435901v1.full -->
<!-- Johnson et al. (2020) Multidimensional ground reaction forces and moments from wearable sensor accelerations via deep learning -->

### Smooth curves (homoscedasticity, normal error, no trend)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

### Smooth curves with nonlinear error trend

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth_trend", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

#### Heteroscedasticity across the signal length

<!-- Smooth curves with non-gaussian (Weibull) error (no trend) -->

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "non_gaussian", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

#### Smooth curves with (missing) peaks

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "shock", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

#### Curves phase shifted in x-axis direction

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "shift", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```


### Cross validation

Leave-one out method to estimate the achieved coverage

Prediction intervals are calculated from size.dataset = number.subjects - 1 and then validated against the left out subjects curves for five different methods:

* FLoA~RCB_v1~   : n = length(subjects) random strides from all strides

* FLoA~RCB_v2~   : One random stride per subject

* FLoA~RCB_v3~   : Fetch a SINGLE random stride from all strides

* FLoA~Roislien~ : Roislien approach (Get one random stride from each subject ONCE and boot-
       strap the resulting sample (of length (n=length(subjects))
       
* FLoA~Point~ : Pointwise B & A Limits of Agreement

<!-- $$ -->
<!-- RMSE_{delta} = \sqrt{\frac{1}{N}(target_1 - pedal_1)^2 + (target_2 - pedal_2)^2 + ... + (target_N - pedal_N)^2} -->
<!-- $$ -->

<!-- $$ -->
<!-- RMSE_{pedal} = \sqrt{\frac{1}{n}[(pedal_1 - \mu)^2 + (m_2 - \mu)^2 + ... + (pedal_N - \mu)^2]}, where \mu = \frac{1}{N}(pedal_1 + ... + pedal_N) -->
<!-- $$ -->

<!-- $$ -->
<!-- Z = \{z_1, z_2, z_3,...,z_i\} -->
<!-- $$ -->

# Results

### Functional limits of agreement

```{r echo = FALSE, warning = FALSE, message = FALSE}
n.boot <- 400

data <- example_data(dat = "smooth", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")

data <- example_data(dat = "smooth_trend", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")

data <- example_data(dat = "non_gaussian", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")

data <- example_data(dat = "shock", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")

data <- example_data(dat = "shift", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")
```

### Leave-one (curve) out cross validation

Displayed are boxes for five different methods:

* 1. FLoA~RCB_v1~ : n = length(subjects) random strides from all strides

* 2. FLoA~RCB_v2~ : One random stride per subject

* 3. FLoA~RCB_v3~ : Fetch a SINGLE random stride from all strides

* 4. FLoA~Roislien~ : Roislien approach (Get one random stride from each subject ONCE and boot-
       strap the resulting sample (of length (n=length(subjects))

* 5. FLoA~Point~ : Pointwise B & A Limits of Agreement (SD derived from linear mixed effects model)

Coverages are displayed for two different scenarios:

* a: Percentage of entirely covered curves (all points within the limits)

* b: Mean percentage of curve points within the respective FLoA: The fraction of points along a curve within the limits is calculated and averaged across all curves of a subject.

#### Smooth, wave data (normal error, constant variance, no trend)

a. (coverage of the entire curve)

```{r warning = FALSE, message = FALSE, eval = FALSE}
get_coverage_singlecurve <- function (curve, floa) {
  # Curve is a numerical vector
  # floa is a "matrix" "array" containing "lower" and "upper" limits

  # Calculate coverage (entire curves within the percentile boundaries)
  # ----------------------------------------------------------------------------
  lwr.bnd <- floa["lower", ]
  upr.bnd <- floa["upper", ]

  outside <- 0

  # Compare curves with upper and lower boundaries
  below.thresh <- curve < lwr.bnd
  above.thresh <- curve > upr.bnd

  points.outside <- sum(above.thresh) + sum(below.thresh)

  coverage <- ifelse(points.outside > 0, 0, 1)

  return(coverage)
}
```

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth", dir.data)

# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.singlecurve <- singlecurve_coverage(data, n.boot)

# Calculate percent from counts
calculate_percent <- function(data) {
  percentage <- (sum(data) / length(data))
  percentage <- round(percentage, digits = 2)
}

covered.curves.percent <- apply(cover.cross.singlecurve, 2, calculate_percent)
covered.curves.percent <- data.frame(t(covered.curves.percent))
colnames(covered.curves.percent) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

knitr::kable(covered.curves.percent, caption = "Percentages of entire covered curves")

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross <- crossval_coverage(data, n.boot)
# # colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")
# 
# plot_cov_ver(cover.cross)
```

b. (mean percentage of covered curve points)

```{r warning = FALSE, message = FALSE, eval = FALSE}
get_coverage_singlecurve_fraction <- function (curve, floa) {
  # Curve is a numerical vector
  # floa is a "matrix" "array" containing "lower" and "upper" limits

  # Calculate coverage (curve points within the percentile boundaries)
  # ----------------------------------------------------------------------------
  lwr.bnd <- floa["lower", ]
  upr.bnd <- floa["upper", ]

  outside <- 0

  # Compare difference curves with upper and lower boundaries
  below.thresh <- curve$value < lwr.bnd
  above.thresh <- curve$value > upr.bnd

  points.outside <- sum(above.thresh) + sum(below.thresh)

  # Divide by 101 to adjust for 101 data points
  coverage <- 1 - (points.outside / 101)
  coverage <- round(coverage, digits = 2)

  return(coverage)
}
```


```{r echo = FALSE, warning = FALSE, message = FALSE}
# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.fraction.singlecurve <- singlecurve_coverage_fraction(data, n.boot)
plot_cov_ver(cover.cross.fraction.singlecurve)

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)
# plot_cov_ver(cover.cross.fraction)
```

#### Curves with nonlinear trend (constant variance)

a. (coverage of the entire curve)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth_trend", dir.data)

# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.singlecurve <- singlecurve_coverage(data, n.boot)

# Calculate percent from counts
calculate_percent <- function(data) {
  percentage <- (sum(data) / length(data))
  percentage <- round(percentage, digits = 2)
}

covered.curves.percent <- apply(cover.cross.singlecurve, 2, calculate_percent)
covered.curves.percent <- data.frame(t(covered.curves.percent))
colnames(covered.curves.percent) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

knitr::kable(covered.curves.percent, caption = "Percentages of entire covered curves")

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross <- crossval_coverage(data, n.boot)
# # colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")
# 
# plot_cov_ver(cover.cross)
```

b. (mean percentage of covered curve points)

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.fraction.singlecurve <- singlecurve_coverage_fraction(data, n.boot)
plot_cov_ver(cover.cross.fraction.singlecurve)

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)
# plot_cov_ver(cover.cross.fraction)
```

#### Curves with non-gaussian (Weibull distributed) error (no trend)

a. (coverage of the entire curve)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "non_gaussian", dir.data)

# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.singlecurve <- singlecurve_coverage(data, n.boot)

# Calculate percent from counts
calculate_percent <- function(data) {
  percentage <- (sum(data) / length(data))
  percentage <- round(percentage, digits = 2)
}

covered.curves.percent <- apply(cover.cross.singlecurve, 2, calculate_percent)
covered.curves.percent <- data.frame(t(covered.curves.percent))
colnames(covered.curves.percent) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

knitr::kable(covered.curves.percent, caption = "Percentages of entire covered curves")

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross <- crossval_coverage(data, n.boot)
# # colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")
# 
# plot_cov_ver(cover.cross)
```

b. (mean percentage of covered curve points)

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.fraction.singlecurve <- singlecurve_coverage_fraction(data, n.boot)
plot_cov_ver(cover.cross.fraction.singlecurve)

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)
# plot_cov_ver(cover.cross.fraction)
```

#### Curves with (missing) peaks

a. (coverage of the entire curve)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "shock", dir.data)

# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.singlecurve <- singlecurve_coverage(data, n.boot)

# Calculate percent from counts
calculate_percent <- function(data) {
  percentage <- (sum(data) / length(data))
  percentage <- round(percentage, digits = 2)
}

covered.curves.percent <- apply(cover.cross.singlecurve, 2, calculate_percent)
covered.curves.percent <- data.frame(t(covered.curves.percent))
colnames(covered.curves.percent) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

knitr::kable(covered.curves.percent, caption = "Percentages of entire covered curves")

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross <- crossval_coverage(data, n.boot)
# # colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")
# 
# plot_cov_ver(cover.cross)
```

b. (mean percentage of covered curve points)

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.fraction.singlecurve <- singlecurve_coverage_fraction(data, n.boot)
plot_cov_ver(cover.cross.fraction.singlecurve)

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)
# plot_cov_ver(cover.cross.fraction)
```

#### Curves phase shifted in x-axis direction

a. (coverage of the entire curve)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "shift", dir.data)

# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.singlecurve <- singlecurve_coverage(data, n.boot)

# Calculate percent from counts
calculate_percent <- function(data) {
  percentage <- (sum(data) / length(data))
  percentage <- round(percentage, digits = 2)
}

covered.curves.percent <- apply(cover.cross.singlecurve, 2, calculate_percent)
covered.curves.percent <- data.frame(t(covered.curves.percent))
colnames(covered.curves.percent) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

knitr::kable(covered.curves.percent, caption = "Percentages of entire covered curves")

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross <- crossval_coverage(data, n.boot)
# # colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")
# 
# plot_cov_ver(cover.cross)
```

b. (mean percentage of covered curve points)

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Leave-one curve out approach
# ------------------------------------------------------------------------------
cover.cross.fraction.singlecurve <- singlecurve_coverage_fraction(data, n.boot)
plot_cov_ver(cover.cross.fraction.singlecurve)

# # Leave-one subject out approach
# # ------------------------------------------------------------------------------
# cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)
# plot_cov_ver(cover.cross.fraction)
```

# Discussion

<!-- Anwendung der Argumentation der Reviewer* auf die "Roislien_FLoA": In Roislien et al. werden 95% Konfidenzintervalle gebootstrapped. The authors are producing the arithmetic mean curves, the variability of which will reflect variation between means. It is the variability of individual values that is relevant, and the limits will be too narrow by a factor of approximately the square root of n.subj. -->

It might be beneficial to work with functional data as suggested by Lenhoff et al. (1999) or Roislien et al. (2012) when e. g. interpolating data to a standardized length. This approach, however, wasn't necessary in this paper, since the synthetic data were created from functions and were designed to all have equal length (101 data points).

Wir empfehlen die Daten zu plotten. Am Ende ist jeder Datensatz bzw. jede Fragestellung unterschiedlich und ein pauschaler coverage value wird der Qualität des Messgeräts (in Bezug auf die jeweilige Fragestellung möglicherweise nicht gerecht).

<!-- Limitations of curvewise intervals -->
<!-- https://mjskay.github.io/ggdist/articles/lineribbon.html -->
<!-- One challenge with curvewise intervals is that they can tend to be very conservative, especially at moderate-to-large intervals widths. -->
<!-- Notice how noisy the curvewise intervals are. In addition, because a number of curves tend to start low and end high (or vice versa), above 50%, the bands rapidly expand to cover almost all of the curves in the sample, regardless of coverage level. -->
<!-- In general I have found that there is no one method that consistently works well on all datasets. No matter the method, intervals often become problematic above 50%, hence the default .width for curve_interval() is 0.5 (unlike the default for point_interval(), which is 0.95). In any case, caution when using these intervals is advised. -->

<!-- Welcher coverage Strategie ist besser (Entire curve (all-or-nothing) vs. percent-of-curve)? -->
<!-- Denkansatz aus Juul et al. (2020) -->
<!-- At the same time, however, it is clear that there is still important research to be done. We have suggested different ways of ranking centrality of ensemble curves. Each rank- -->
<!-- ing has its merits: For example, the all-or-nothing ranking penalizes curves which are shaped differently than other ensemble curves and a weighted ranking can reward curves that are central in the very near future. A thorough investigation of these merits and trade-offs is needed. Until this has been investigated, we encourage researchers to be creative and mindful about the problems that each statistical method could have. And to communicate this uncertainty openly to decision makers. -->

<!-- Für die hier untersuchten biomechanischen Signale nicht so relevant, aber an sich ein sehr interessantes Feature ist das hier: -->
<!-- Leicht abgewandelt nach Juul et al. (2020): -->
<!-- In addition to the ranking methods (for finding quantiles/percentiles) mentioned above, one can rank the curves according to some feature of interest. In Fig. 2E we show the curve box plots obtained when we rank curves according to their projected maximum values of newly hospitalized cases in a single day; in other words, the median projected peak value received -->
<!-- the highest centrality. -->

<!-- Interessant beim Juul Paper ist auch der Umgang mit ("abgeschnittenen") Extremwerten: -->
<!-- Here we have argued that computing confidence intervals using fixed-time descriptive systematically suppresses trajectory extremes. This is natural. -->
<!-- Fixed-time descriptive statistics are designed to show the least extreme predictions on a given date, not to take entire curves into account. -->

# Conclusion

<!-- Aus Juul et al. (2020) -->
<!-- In summary, when making forecasts of epidemic trajectories, it is important to represent the resulting curve ensembles in a way that captures the quantities of interest -->
<!-- in an intuitive way. -->

