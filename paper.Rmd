---
title: "Asymmetric continuous prediction bands for assessing the validity of spatiotemporally smooth biomechanical curve data"
author:
  - name: Daniel Koska
    email: daniel.koska@hsw.tu-chemnitz.de
    affiliation: Chemnitz University of Technology, Research Methodology and Data Analysis in Biomechanics
    footnote: Corresponding Author
  - name: Doris Oriwol
    email: doris.oriwol@partner.kit.edu
    affiliation: Karlsruhe Institute of Technology
  - name: Christian Maiwald
    email: christian.maiwald@hsw.tu-chemnitz.de
    affiliation: Chemnitz University of Technology, Research Methodology and Data Analysis in Biomechanics
# address:
#   - code: Chemnitz University of Technology
#     address:  Thüringer Weg 11, 09126 Chemnitz
#   - code: Another University
#     address: Department, Street, City, State, Zip
abstract: |
  Paper als short communication / technical note?

date: "`r format(Sys.time(), '%d %B, %Y')`"
journal: "An awesome journal"
# bibliography: sendaFAB.bib
# output: rticles::elsevier_article
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
---

### TODO
- Simulierte Daten durch Funktionen ersetzen? Eigentlich aktuell eher unnötig, da die Zeitreihen ja eigentlich bereits mithilfe von Funktionen modelliert wurden. Zudem wird nicht interpoliert, abgeleitet o. Ä. ... letztlich für die aktuelle Idee des Papers eher ungeeignet (plus man umgeht Probleme wie mögliche Glättungsartefakte, oder die Definition eines functional mean).
- Falls doch FDA, dann evtl. als eigenständiges Skript forken
- .bib file anlegen
- Begriff spatiotemporal überdenken (wie ist das im Kontext FDA definiert)
- Ratkowsky besorgen

### TODO read
- Horvath, Kokoszka & Reeder (2013) Estimation of the mean of functional time series and a two-sample problem
- Generell die Kooperationspaper von Hörmann und Kokoszka zu FDA Statistik,insbesondere 'Inference for Functional Data with Applications' (2012)
- Ratkowsky 1990 Handbook of nonlinear regression models


```{r echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)

dir.script <- "C:/Users/Daniel/Desktop/tmp/floa/R"
dir.data <- "C:/Users/Daniel/Desktop/tmp/floa/R/examples"

source(paste0(dir.script, "/example_data.R"))
source(paste0(dir.script, "/pick_subwise_curves.R"))
source(paste0(dir.script, "/draw_clusters.R"))
source(paste0(dir.script, "/functional_mean.R"))
source(paste0(dir.script, "/functional_sd.R"))
source(paste0(dir.script, "/boot_mean_sd.R"))
source(paste0(dir.script, "/floa_rcb.R"))
source(paste0(dir.script, "/floa_point.R"))
source(paste0(dir.script, "/floa_roislien.R"))
source(paste0(dir.script, "/plot_loa.R"))
source(paste0(dir.script, "/get_coverage.R"))
source(paste0(dir.script, "/get_coverage_fraction.R"))
source(paste0(dir.script, "/crossval_coverage.R"))
source(paste0(dir.script, "/crossval_coverage_fraction.R"))
source(paste0(dir.script, "/plot_cov_ver.R"))
```

# Introduction

Having a good estimate of the amount of measurement error is a basic requirement when analyzing movement data. The classic linear error model describes measurement error as the sum of systematic and random deviations from the true value. A complete characterization of the measurement error, however, is complex, as different sources of variance (within, between) and noise must be considered. The analysis of sensor data is further complicated by the fact that the output of many measurement systems in biomechanics - such as e. g. joint angles, ground reaction forces, or center of mass trajectories - is available in the form of continuous signals (curve data) within which the error can vary over time.

Therefore, when analyzing the validity of curve data, the entire length of the curve should be accounted for. This can be accomplished e. g. by applying statistics (e. g. standard deviation) separately to all points of the curve [Pini et al. (2019)]. The pointwise approach allows for an error representation over the entire domain, but ignores the fact, that the sampling points of biomechanical curves are locally correlated (Deluzio & Astephen, 2007; Pataky, 2010). From a statistical point of view, this implies that the random error component is likely underrepresented when the (nonzero) covariance term of adjacent sampling points is not accounted for (Lenhoff et al., 1999). In addition, most traditional point statistics are either not suitable (e.g. statistical tests of mean differences), or are only suitable to a limited extent (e.g. product-moment correlation) to quantify the agreement between two measurement systems (Bland and Altman, 1986).

One possibility to address the problem of pointwise analysis is to treat entire curves as (functional) objects rather than as a series of unconnected points [Ramsay & Silverman, 2005]. Røislien et al. (2012) presented a method that extends the Limits of Agreement (LoA) suggested by Bland & Altman to functional data by approximating measurement signals using Fourier series. These function are used to calculated functional difference curves which are then bootstrapped to obtain 95% confidence intervals. The approach accounts for the autocorrelation characteristic in the signal and at the same time allows for an intuitive description of the systematic and random error terms in the original sampling space (e.g. actual degrees). Røislien et al., however, make a number of strong assumptions, which are often not or only partially fulfilled in empirical data sets:

(i) The error distribution is approximately normal.

The construction of symmetric prediction bands as described in Røislien et al. [10] relies on normally distributed measurement data. Non-normality, however, is often present when e.g. sensor values are truncated because of limited range or resolution, or because values drift over time.

(ii) Time-continuous differences of two measurement systems and the corresponding averages must be uncorrelated.

This assumption is violated considerably in many validation data sets [12]. E.g., Atallah et al. [13] found that the agreement between an ear-worn accelerometer and a force-plate instrumented treadmill was associated with gait speed when estimating gait cycle duration. When validating a foot-worn gyroscope for treadmill running, Koska & Maiwald [14] observed a linear error trend that was associated with foot strike behavior.

(iii) Aus einem Datensatz bestehend aus einer Kurve eines Probanden werden 95% Konfidenzintervalle gebootrapped, die ein geeignetes Maß für die estimation uncertainty darstellen.

In jeder Iteration des Bootstrap wird eine arithmetische mean curve gebildet. ... the limits will be too narrow by a factor of approximately the square root of n.subj.

<!-- Prädiktionsbänder sind weiter als Konfidenzbänder ... und eigentlich will man ja Prädiktionsbänder, oder!?! -->

<!-- Anwendung der Argumentation der Reviewer* auf die "Roislien_FLoA": In Roislien et al. werden 95% Konfidenzintervalle gebootstrapped. The authors are producing the arithmetic mean curves, the variability of which will reflect variation between means. It is the variability of individual values that is relevant, and the limits will be too narrow by a factor of approximately the square root of n.subj. -->

<!-- Formulierung aus dem alten Skript -->
<!-- (iii) Measurement uncertainty can be estimated adequately by bootstrapping the error variance from a sample that consists of a single curve per subject. -->
<!-- If no repeated measurements are included in the sample, an essential error component (intra-individual variance) is ignored. The bootstrap – like any other statistical procedure – cannot solve this problem, as it can only estimate proportions of variance that are contained in the sample. -->

<!-- Weiterer ("zugehöriger) Reviewerkommentar: -->
<!-- In looking at limits of agreement, the authors are considering the extent of within pair variation. However, in looking at the point wise limits of agreement, these are defined to incorporate a between pair standard deviation. Even if it was appropriate to utilise between pair variability, the authors should note that it is variances that are additive and not SDs. Perhaps the authors' lack of understanding comes from their erroneous assumption (iii).  -->

Olsen et al. [15] examined the validity of an inertial measurement unit (IMU) for analyzing the spatial displacement of the distal limbs of horses using functional LoA (FLoA). Their approach differs from Røislien et al. [10] in that initially linear mixed models were fitted to the repeated measurements of different horses. Functional data estimates of the mean trends in the strides were then determined from the model fits and FLoA were calculated. This procedure accounts for repeated measurements and eliminates the need to emulate empirical curves with Fourier series or splines. The FLoA computed this way performed well in comparison with traditional Bland & Altman LoA. The procedure, however, is subject to the same restrictions as any conventional linear regression model and is therefore limited when model assumptions are violated.

The principal object of this paper was to demonstrate a method for constructing continuous prediction intervals from curve data that addresses the aforementioned issues (analyzing the whole curve, including repeated measures, (parametric) model assumptions), and thus enables a more appropriate characterization of the agreement between measurement curves on the basis of easy to interpret error bands (i.e. LoA in the actual measurement unit).

We use synthetic time series data sets containing typical curves of biomechanical measurement systems to illustrate the method and compare the resulting prediction bands to pointwise Bland & Altman parameters and LoA constructed similar to the method described in Roislien et al. (2012). For model validation, the respective LoA are cross validated using a leave-one-out approach.

# Methods

## Limits of agreement methods

FLoA derived by different methods are compared

* Randomized Cluster Bootstrap (FLoA~RCB~)
  + n = length(subjects) random strides from all strides (FLoA~RCB_v1~)
  + One random stride per subject (FLoA~RCB_v2~)
  + Fetch a SINGLE random stride from all strides (FLoA~RCB_v3~)

* Pointwise Gaussian intervals (FLoA~Point~)
  + FLoA~Point~ are calculated using mean and standard deviations (derived from linear mixed effects models)

* Roislien-like intervals (FLoA~Roslien~)
  + (Get one random stride from each subject ONCE and boot-
       strap the resulting sample (of length (n=length(subjects))


```{r echo = FALSE, warning = FALSE, message = FALSE}
# Wrapper function for example data sets. Function arguments:
#
# (* Empirical validation data: "imu_mc")
# * Smooth, wave data (normal error, constant variance, no trend): "smooth"
# * Smooth wave data with nonlinear trend (constant variance): "smooth_trend"
# * Data with non-gaussian (Weibull distributed) error (no trend): "non_gaussian"
# * Data with shock peaks (no bias, no trend): "shock"

data <- example_data(dat = "smooth", dir.data)

# Mean and SD are calculated across all strides (and subjects).
# No bootstrap or other resampling strategies are applied.
floa.point <- floa_point(data)
```

## Validation

### Synthetic data sets

Für die Validierung der Methode wurden Zeitreihen simuliert, die gängigen biomechanischen Messsignalen qualitativ ähneln und typische Fehlercharakteristika abbilden. Die verschiedenen Zeitreihen wurden als Fourierreihen aus additiv überlagerten Sinusfunktionen modelliert. Jeder Datensatz enthält n.strides = 10 Kurven von n.subj = 11 fiktiven Probanden. Jede Kurve besteht aus t = 101 Datenpunkten.

```{r warning = FALSE, message = FALSE, eval = FALSE}
n.subj <- 11
n.ts <- 100

t <- seq(0, 100)

for (subject.idx in 1:n.subj) {

  # Subjectwise parameters
  offset.mean <- runif(1, min = -0.5, max = 0.5)

  a.sd <- runif(1, min = 0.05, max = 0.15)
  b.sd <- runif(1, min = 0.0001, max = 0.002)

  for (stride.idx in 1:(n.strides)) {

    a1.device1 <- rnorm(1, mean = 3, sd = a.sd)
    a1.device2 <- rnorm(1, mean = 3, sd = a.sd)
    a2.device1 <- 0.08
    a2.device2 <- 0.08
    b1.device1 <- rnorm(1, mean = 0.06, sd = b.sd)
    b1.device2 <- rnorm(1, mean = 0.06, sd = b.sd)
    b2.device1 <- rnorm(1, mean = 0.58, sd = b.sd)
    b2.device2 <- rnorm(1, mean = 0.58, sd = b.sd)
    c <- 2

    sine.1 <- a1.device1 * sin(b1.device1 * t) ^ (c + 3)
    sine.2 <- a2.device1 * sin(b2.device1 * t)
    sine.3 <- a1.device2 * sin(b1.device2 * t) ^ (c + 3)
    sine.4 <- a2.device2 * sin(b2.device1 * t)

    offset <- rnorm(1, offset.mean, 0.05)
    
    # Model (i). (ii), or (iii)
    # ...
```

* (i) Smooth curves with normally distributed errors, constant variance, and no trend. Within and between subject differences between curves were modeled using random systematic offset (bias) and random variation of curve parameters. The data resembles e. g. kinematic joint angle curves captured by camera-based measurement systems or goniometers.

```{r warning = FALSE, message = FALSE, eval = FALSE}
# ...

# Model (i)
device.1 <- sine.1 + sine.2
device.2 <- offset + sine.3 + sine.4
```

* (ii) Smooth curves created using the same model and parameters as in (i), but with a nonlinear trend underlying one of the two signals. A similar trend (drift) behavior can be observed in e. g. when the same joint angle curves are calculated by integrating the signal of gyropscopes applied to measure the movement.

```{r warning = FALSE, message = FALSE, eval = FALSE}
# ...
trend <- (1 / 100000) * seq(0.5, 50.5, 0.5)^3

# Model (ii)
device.1 <- sine.1 + sine.2
device.1 <- offset + sine.3 + sine.4 + trend
```

* (iii) Smooth curves created using the same model and parameters as in (i), but Weibulll distributed random curve (amplitude) parameters. This introduces non-gaussian measurement error (asymmetric/skewed distribution of differences around the mean difference).

```{r warning = FALSE, message = FALSE, eval = FALSE}
# ...
a1.device2 <- rnorm(n = 1,
                    mean = rweibull(1, shape = 1.5, scale=1) - factorial(1/1.5), # factorial() used to center around 0
                    sd = a.sd)
# ...
sine.3 <- a1.device2 * sin(b1.device2 * t) ^ (c + 3)
# ...

# Model (iii)
device.1 <- sine.1 + sine.2
device.2 <- offset + sine.3 + sine.4
```



### Smooth curves (normal error, constant variance, no trend)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

### Smooth curves nonlinear error trend

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth_trend", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

#### Smooth curves with non-gaussian (Weibull) error (no trend)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "non_gaussian", dir.data)

data.single.mc <- subset(data, device == "MC")
data.single.imu <- subset(data, device == "IMU")

PLOT <- ggplot(data = data.single.mc, aes(x = frame, y = value, group = strideID)) + # , colour = subjectID
  geom_line(alpha = 0.7) +
  geom_line(data = data.single.imu, aes(x = frame, y = value, group = strideID, col = "red"), alpha = 0.3) + #, colour = subjectID
  labs(x = "Time-normalized signal [%]", y = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 20),
        axis.title.x = element_text(size = 22),
        axis.text.y = element_text(size = 20),
        axis.title.y = element_text(size = 22),
        legend.position = "none")

PLOT
```

### Cross validation

Leave-one out method to estimate the achieved coverage

Prediction intervals are calculated from size.dataset = number.subjects - 1 and then validated against the left out subjects curves for five different methods:

* FLoA~RCB_v1~   : n = length(subjects) random strides from all strides

* FLoA~RCB_v2~   : One random stride per subject

* FLoA~RCB_v3~   : Fetch a SINGLE random stride from all strides

* FLoA~Roislien~ : Roislien approach (Get one random stride from each subject ONCE and boot-
       strap the resulting sample (of length (n=length(subjects))
       
* FLoA~Point~ : Pointwise B & A Limits of Agreement

<!-- $$ -->
<!-- RMSE_{delta} = \sqrt{\frac{1}{N}(target_1 - pedal_1)^2 + (target_2 - pedal_2)^2 + ... + (target_N - pedal_N)^2} -->
<!-- $$ -->

<!-- $$ -->
<!-- RMSE_{pedal} = \sqrt{\frac{1}{n}[(pedal_1 - \mu)^2 + (m_2 - \mu)^2 + ... + (pedal_N - \mu)^2]}, where \mu = \frac{1}{N}(pedal_1 + ... + pedal_N) -->
<!-- $$ -->

<!-- $$ -->
<!-- Z = \{z_1, z_2, z_3,...,z_i\} -->
<!-- $$ -->

# Results

### Functional limits of agreement

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth", dir.data)
n.boot <- 400
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")

data <- example_data(dat = "smooth_trend", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")

data <- example_data(dat = "non_gaussian", dir.data)
floa <- floa_rcb(data, n.boot, ver = "v2")
plot_loa(data, floa, central.tendency = "mean")
```

### Leave-one-out cross validation

Displayed are boxes for five different methods:

* 1. FLoA~RCB_v1~ : n = length(subjects) random strides from all strides

* 2. FLoA~RCB_v2~ : One random stride per subject

* 3. FLoA~RCB_v3~ : Fetch a SINGLE random stride from all strides

* 4. FLoA~Roislien~ : Roislien approach (Get one random stride from each subject ONCE and boot-
       strap the resulting sample (of length (n=length(subjects))

* 5. FLoA~Point~ : Pointwise B & A Limits of Agreement

Coverages are displayed for two different scenarios:

* a: Percentage of entirely covered curves (all points within the limits): Each box consists of n~ = 11 values (one percentage for each subject).

* b: Mean percentage of curve points within the respective FLoA: The fraction of points along a curve within the limits is calculated and averaged across all curves of a subject.

#### Smooth, wave data (normal error, constant variance, no trend)

a. (coverage of the entire curve)

```{r warning = FALSE, message = FALSE, eval = FALSE}
lwr.bnd <- floa["lower", ]
upr.bnd <- floa["upper", ]

# ...

for (stride.idx in stride.indices){
    curve <- subset(device.diff, strideID == stride.idx)

    # Compare difference curves with upper and lower boundaries
    below.thresh <- curve$value < lwr.bnd
    above.thresh <- curve$value > upr.bnd

    points.outside <- sum(above.thresh) + sum(below.thresh)

    # Check if the entire curve is within the limits
    if (points.outside > 0) {
      outside <- outside + 1
    }
  }

coverage <- 1 - (outside / n.strides)
```

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth", dir.data)

cover.cross <- crossval_coverage(data, n.boot)
# colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

plot_cov_ver(cover.cross)

# knitr::kable(cover.cross, caption = "Percentages (entire curve covered) for every leave one out fold")
```

b. (mean percentage of covered curve points)

```{r warning = FALSE, message = FALSE, eval = FALSE}
lwr.bnd <- floa["lower", ]
upr.bnd <- floa["upper", ]

# ...

for (stride.idx in stride.indices){

  curve <- subset(device.diff, strideID == stride.idx)

  # Compare difference curves with upper and lower boundaries
  below.thresh <- curve$value < lwr.bnd
  above.thresh <- curve$value > upr.bnd

  # Mean percentage of points outside the limits
  inside <- below.thresh == above.thresh
  inside.thresh.perc <- c(inside.thresh.perc,
                          mean(sum(inside[TRUE]) / length(curve$value)))
}
```


```{r echo = FALSE, warning = FALSE, message = FALSE}
# Display cross validation coverages (mean fraction of the curve that is covered) across methods
cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)

plot_cov_ver(cover.cross.fraction)
```

#### Smooth wave data with nonlinear trend (constant variance)

a. (coverage of the entire curve)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "smooth_trend", dir.data)

cover.cross <- crossval_coverage(data, n.boot)
# colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

plot_cov_ver(cover.cross)

# knitr::kable(cover.cross, caption = "Percentages (entire curve covered) for every leave one out fold")
```

b. (mean percentage of covered curve points)

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Display cross validation coverages (fraction of the curve that is covered) across methods
cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)

plot_cov_ver(cover.cross.fraction)
```

#### Data with non-gaussian (Weibull distributed) error (no trend)

a. (coverage of the entire curve)

```{r echo = FALSE, warning = FALSE, message = FALSE}
data <- example_data(dat = "non_gaussian", dir.data)

cover.cross <- crossval_coverage(data, n.boot)
# colnames(cover.cross) <- c("RCB_1", "RCB_2", "RCB_3", "Roislien_4", "Point_5")

plot_cov_ver(cover.cross)

# knitr::kable(cover.cross, caption = "Percentages (entire curve covered) for every leave one out fold")
```

b. (mean percentage of covered curve points)

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Display cross validation coverages (fraction of the curve that is covered) across methods
cover.cross.fraction <- crossval_coverage_fraction(data, n.boot)

plot_cov_ver(cover.cross.fraction)
```

# Discussion

<!-- Anwendung der Argumentation der Reviewer* auf die "Roislien_FLoA": In Roislien et al. werden 95% Konfidenzintervalle gebootstrapped. The authors are producing the arithmetic mean curves, the variability of which will reflect variation between means. It is the variability of individual values that is relevant, and the limits will be too narrow by a factor of approximately the square root of n.subj. -->

It might be beneficial to work with functional data as suggested by Lenhoff et al. (1999) or Roislien et al. (2012) when e. g. interpolating data to a standardized length. This approach, however, wasn't necessary in this paper, since the synthetic data were created from functions and were designed to all have equal length (101 data points).

# Conclusion

